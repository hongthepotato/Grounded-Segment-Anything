# Docker Compose configuration for Training Job Manager
#
# Services:
#   - redis: Redis for job queue and state
#   - api: FastAPI server for REST/WebSocket
#   - worker: Training worker (GPU required)
#
# Usage:
#   # Start Redis and API only (for development)
#   docker-compose up redis api
#
#   # Start everything (requires NVIDIA runtime)
#   docker-compose up
#
#   # Scale workers
#   docker-compose up --scale worker=2

version: "3.8"

services:
  # Redis for job queue and state persistence
  redis:
    image: redis:7-alpine
    container_name: training-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # FastAPI server
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: api
    container_name: training-api
    ports:
      - "8080:8080"
    environment:
      - REDIS_URL=redis://redis:6379
      - CORS_ORIGINS=*
      - API_HOST=0.0.0.0
      - API_PORT=8080
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./experiments:/app/experiments
      - ./data:/app/data
    command: uvicorn api.app:app --host 0.0.0.0 --port 8080
    restart: unless-stopped

  # Training worker (GPU)
  worker:
    build:
      context: .
      dockerfile: Dockerfile
      target: worker
    environment:
      - REDIS_URL=redis://redis:6379
      - CUDA_VISIBLE_DEVICES=0
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./experiments:/app/experiments
      - ./data:/app/data
      - ./configs:/app/configs
    # GPU access (requires nvidia-docker2)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: python -m ml_engine.jobs --gpu 0
    restart: unless-stopped

volumes:
  redis-data:





