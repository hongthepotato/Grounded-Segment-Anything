# SAM LoRA Fine-tuning Configuration
# This config contains ONLY SAM-specific settings.
# Shared training params are in teacher_training.yaml

# Training strategy
training:
  # Enable SAM-HQ style training: 1 random object per image
  # This significantly reduces memory usage and matches original SAM-HQ training
  single_object_sampling: true

# Model configuration
model:
  name: "sam"
  base_checkpoint: "data/models/pretrained/sam_hq_vit_h.pth"
  model_type: "vit_h"     # Options: vit_h, vit_l, vit_b

# CRITICAL: SAM architecture has different attention layer names:
#   - image_encoder uses: attn.qkv (combined), attn.proj
#   - mask_decoder uses: self_attn.q_proj, self_attn.k_proj, etc.
# Since these names are DIFFERENT, using "attn.qkv" will ONLY match image_encoder!
lora:
  enabled: true
  r: 16                      # Higher rank for encoder (more capacity needed)
  lora_alpha: 32             # Scaling factor (alpha/r = 2 is common)
  target_modules:            # Apply LoRA ONLY to image_encoder attention layers
    - "attn.qkv"             # Combined Q/K/V projection (only in image_encoder ViT)
    - "attn.proj"            # Output projection (only in image_encoder ViT)
  lora_dropout: 0.1          # Standard dropout for encoder
  bias: "none"

# Training mode for each component
# Options: "frozen" | "lora" | "full"
#   - frozen: No training, all parameters frozen
#   - lora: LoRA adapters trainable, base weights frozen (requires target_modules match)
#   - full: All parameters trainable (full fine-tuning)
image_encoder_mode: "frozen"     # LoRA on ViT backbone (~1.5M trainable)
prompt_encoder_mode: "frozen"  # Completely frozen (~3.8M frozen)
mask_decoder_mode: "full"      # Full fine-tune (~4.1M trainable)

# Model-specific training settings
# Use different LRs for LoRA vs full fine-tune
learning_rate: 1.0e-4        # Base LR for LoRA adapters
mask_decoder_lr_multiplier: 0.1  # Lower LR for mask_decoder (pretrained weights)

# Prompt strategy during fine-tuning
prompt_type: "boxes"         # Use GT boxes as prompts (auto-gen if masks-only)
multimask_output: true       # Output multiple mask candidates

# Evaluation metric
evaluation:
  metric: "mask_IoU"         # Primary metric for SAM model selection
