# Teacher Fine-tuning Implementation Guide

## Overview

This implementation provides a complete teacher model fine-tuning system with:

‚úÖ **Data-driven architecture**: Automatically detects annotation types and loads appropriate models  
‚úÖ **LoRA integration**: Memory-efficient fine-tuning (3-10x less memory)  
‚úÖ **Auto-configuration**: No manual config editing required  
‚úÖ **Multi-model support**: Grounding DINO, SAM, or both  
‚úÖ **Characteristic-based augmentation**: Intelligent augmentation selection  
‚úÖ **Production-ready**: Comprehensive error handling, logging, checkpointing  

## Project Structure

```
platform/
‚îú‚îÄ‚îÄ cli/                           # Command-line interface
‚îÇ   ‚îú‚îÄ‚îÄ validate_dataset.py       # Dataset validation and splitting
‚îÇ   ‚îú‚îÄ‚îÄ train_teacher.py          # Teacher fine-tuning
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                  # CLI utilities
‚îÇ
‚îú‚îÄ‚îÄ ml_engine/                    # ML training engine
‚îÇ   ‚îú‚îÄ‚îÄ data/                     # Data processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inspection.py         # Dataset inspection (data-driven)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loaders.py            # COCO dataset loaders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validators.py         # Validation + auto-bbox generation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py      # Multi-model preprocessing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/teacher/           # Teacher models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grounding_dino_lora.py  # Grounding DINO with LoRA
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sam_lora.py             # SAM with LoRA
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ training/                 # Training components
‚îÇ       ‚îú‚îÄ‚îÄ teacher_trainer.py    # Main trainer (data-driven)
‚îÇ       ‚îú‚îÄ‚îÄ training_manager.py   # Gradient handling, AMP
‚îÇ       ‚îú‚îÄ‚îÄ checkpoint_manager.py # Checkpointing, early stopping
‚îÇ       ‚îú‚îÄ‚îÄ losses.py             # Loss functions
‚îÇ       ‚îî‚îÄ‚îÄ peft_utils.py         # LoRA utilities
‚îÇ
‚îú‚îÄ‚îÄ augmentation/                 # Characteristic-based augmentation
‚îÇ   ‚îî‚îÄ‚îÄ (already implemented)
‚îÇ
‚îú‚îÄ‚îÄ core/                         # Core utilities
‚îÇ   ‚îú‚îÄ‚îÄ config.py                 # Config management
‚îÇ   ‚îú‚îÄ‚îÄ logger.py                 # Logging utilities
‚îÇ   ‚îî‚îÄ‚îÄ constants.py              # Constants and defaults
‚îÇ
‚îú‚îÄ‚îÄ configs/defaults/             # Default configurations
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.yaml        # Model-specific preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ training_dynamics.yaml    # Gradient, AMP, BN settings
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_config.yaml    # Checkpointing settings
‚îÇ   ‚îú‚îÄ‚îÄ teacher_grounding_dino_lora.yaml
‚îÇ   ‚îî‚îÄ‚îÄ teacher_sam_lora.yaml
‚îÇ
‚îú‚îÄ‚îÄ tests/                        # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/                     # Unit tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_data_pipeline.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_preprocessing.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_lora.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_augmentation.py
‚îÇ   ‚îî‚îÄ‚îÄ integration/              # Integration tests
‚îÇ       ‚îî‚îÄ‚îÄ test_teacher_training.py
‚îÇ
‚îú‚îÄ‚îÄ examples/                     # Example scripts
‚îÇ   ‚îî‚îÄ‚îÄ train_teacher_example.py
‚îÇ
‚îî‚îÄ‚îÄ docs/                         # Documentation
    ‚îî‚îÄ‚îÄ CLI_USAGE.md
```

## Key Components

### 1. Data Pipeline (Data-Driven)

**Core principle**: The data structure itself determines pipeline behavior.

```python
from ml_engine.data import inspect_dataset, get_required_models

# Inspect dataset
dataset_info = inspect_dataset(coco_data)
# Returns: {has_boxes, has_masks, num_classes, class_mapping, ...}

# Determine required models (DATA-DRIVEN!)
models = get_required_models(dataset_info)
# If has_boxes ‚Üí ['grounding_dino']
# If has_masks ‚Üí ['sam']
# If both ‚Üí ['grounding_dino', 'sam']
```

No mode enums, no state files - just inspect the data and load what's needed.

### 2. LoRA Fine-tuning

**Partial Freeze + LoRA strategy**:

```python
from ml_engine.models.teacher import load_grounding_dino_with_lora

model = load_grounding_dino_with_lora(
    base_checkpoint='pretrained/groundingdino.pth',
    num_classes=3,
    lora_config={'r': 16, 'lora_alpha': 32}
)

# Result:
# - Backbone: FROZEN ‚ùÑÔ∏è (158M params)
# - Decoder with LoRA: TRAINABLE üî• (2.5M params)
# - Memory: 14GB (vs 47GB full fine-tuning)
# - Saves: Only 19MB LoRA adapters
```

### 3. Multi-Model Preprocessing

Different models need different preprocessing:

```python
from ml_engine.data import create_preprocessor_from_models

# Data-driven: create preprocessor for required models
preprocessor = create_preprocessor_from_models(['grounding_dino', 'sam'])

preprocessed = preprocessor.preprocess_batch(image)
# Returns: {
#     'grounding_dino': (tensor_800x1333, metadata),
#     'sam': (tensor_1024x1024, metadata)
# }
```

Each model gets correctly sized and normalized input automatically.

### 4. Characteristic-based Augmentation

Describe your objects, get appropriate augmentations:

```python
from augmentation import get_augmentation_registry

registry = get_augmentation_registry()
pipeline = registry.get_pipeline(
    characteristics=["changes_shape", "reflective_surface"],
    environment={"lighting": "variable", "camera": "fixed"},
    intensity="medium"
)

augmented = pipeline(image=img, masks=masks, bboxes=boxes)
```

### 5. Training Manager

Handles gradient clipping, mixed precision, batch normalization:

```python
from ml_engine.training import TrainingManager

manager = TrainingManager(
    model=model,
    optimizer=optimizer,
    config_path='configs/defaults/training_dynamics.yaml'
)

# Automatic gradient management
loss_dict = manager.training_step(batch, compute_loss_fn)
```

### 6. Checkpoint Manager

Automatic saving, best model tracking, early stopping:

```python
from ml_engine.training import CheckpointManager

manager = CheckpointManager(
    output_dir='experiments/exp1/teachers/dino_lora',
    config_path='configs/defaults/checkpoint_config.yaml'
)

manager.save_checkpoint(epoch, model, optimizer, metrics)

if manager.should_stop:
    print("Early stopping triggered!")
```

## Usage Examples

### CLI Usage (Recommended)

```bash
# 1. Validate dataset
python cli/validate_dataset.py \
    --data data/raw/annotations.json \
    --split train:0.7,val:0.15,test:0.15 \
    --stratify --seed 42

# 2. Train teachers (auto-detects from data)
python cli/train_teacher.py \
    --data data/raw/train.json \
    --val data/raw/val.json \
    --output experiments/exp1 \
    --batch-size 8 \
    --epochs 50
```

### Programmatic API

```python
from ml_engine.training import TeacherTrainer
from ml_engine.data import load_and_inspect_dataset
from core.config import generate_config

# Inspect dataset
dataset_info = load_and_inspect_dataset('train.json')

# Generate config
config = generate_config(
    default_config_path='configs/defaults/teacher_grounding_dino_lora.yaml',
    dataset_info=dataset_info
)

# Train
trainer = TeacherTrainer(
    train_data_path='train.json',
    val_data_path='val.json',
    image_dir='images/',
    output_dir='experiments/exp1',
    config=config
)

trainer.train()
```

## Data-Driven Design Philosophy

### Traditional Approach (BAD)

```python
# BAD: Mode enums and lookup tables
mode = AnnotationMode.DETECTION_ONLY
config = PIPELINE_CONFIG[mode]

if mode == AnnotationMode.DETECTION_ONLY:
    load_grounding_dino()
elif mode == AnnotationMode.SEGMENTATION_ONLY:
    load_sam()
else:
    load_both()
```

### Our Approach (GOOD)

```python
# GOOD: Data structure drives behavior
dataset_info = inspect_dataset(coco_data)

if dataset_info['has_boxes']:
    load_grounding_dino()
if dataset_info['has_masks']:
    load_sam()
```

**Benefits:**
- No mode enums
- No state files
- No lookup tables
- Simpler, more maintainable
- Easier to extend (add keypoints? Just check `'keypoints' in ann`)

## Auto-Configuration System

### How It Works

1. **Inspect dataset**: Extract `num_classes`, `class_names`, `annotation_mode`
2. **Load defaults**: Read default config template
3. **Merge**: Combine defaults + dataset info + CLI overrides
4. **Save**: Store generated config for reproducibility

### Example

```python
# User runs this:
python cli/train_teacher.py --data train.json --val val.json --batch-size 16

# Platform does this automatically:
dataset_info = inspect_dataset(load_json('train.json'))
# ‚Üí {num_classes: 3, class_names: ['cat', 'dog', 'bird'], ...}

config = load_yaml('configs/defaults/teacher_grounding_dino_lora.yaml')
# ‚Üí {learning_rate: 1e-4, batch_size: 8, ...}

config['num_classes'] = dataset_info['num_classes']  # Auto-fill
config['class_names'] = dataset_info['class_names']  # Auto-fill
config['batch_size'] = 16  # CLI override

save_yaml(config, 'experiments/exp1/teacher_config.yaml')
# ‚Üí Saved for reproducibility
```

**No manual config editing required!**

## Memory Requirements

| Component | Full Fine-tuning | LoRA Fine-tuning |
|-----------|-----------------|------------------|
| Grounding DINO | 47GB | 14.4GB |
| SAM | 20GB+ | 8GB |
| **Total (both)** | **67GB+** | **22.4GB** |
| **Required GPU** | **A100 80GB** | **RTX 3090 24GB** ‚úÖ |

**LoRA enables training on consumer GPUs!**

## Output Structure

```
experiments/exp1/
‚îú‚îÄ‚îÄ teachers/
‚îÇ   ‚îú‚îÄ‚îÄ grounding_dino_lora/      # LoRA adapters (19MB)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.bin
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ best.pth              # Best checkpoint
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ last.pth              # Last checkpoint
‚îÇ   ‚îî‚îÄ‚îÄ sam_lora/                 # LoRA adapters (1.5MB)
‚îÇ       ‚îú‚îÄ‚îÄ adapter_config.json
‚îÇ       ‚îú‚îÄ‚îÄ adapter_model.bin
‚îÇ       ‚îú‚îÄ‚îÄ best.pth
‚îÇ       ‚îî‚îÄ‚îÄ last.pth
‚îú‚îÄ‚îÄ teacher_config.yaml           # Auto-generated config
‚îú‚îÄ‚îÄ metadata.json                 # Experiment metadata
‚îî‚îÄ‚îÄ logs/                         # TensorBoard logs
    ‚îú‚îÄ‚îÄ grounding_dino/
    ‚îî‚îÄ‚îÄ sam/
```

## Testing

Run the test suite:

```bash
# Unit tests
python -m pytest tests/unit/ -v

# Integration tests
python -m pytest tests/integration/ -v

# All tests
python -m pytest tests/ -v
```

## Extending the System

### Adding New Annotation Types

Example: Add keypoint detection support

```python
# 1. Update inspection (automatic)
# Just check for 'keypoints' in annotations - that's it!

if 'keypoints' in ann:
    dataset_info['has_keypoints'] = True
    models.append('pose_model')  # Data-driven!

# 2. Add preprocessing config for pose model
# configs/defaults/preprocessing.yaml:
pose_model:
  input_size: 256
  normalization: {...}

# 3. Add model loader
from ml_engine.models.teacher import load_pose_model

# Total changes: ~10 lines of code
```

No mode enums, no lookup tables, no config templates - just add support for the new field.

## Troubleshooting

### ImportError: No module named 'peft'

```bash
pip install peft>=0.7.0
```

### CUDA Out of Memory

```bash
# Reduce batch size
python cli/train_teacher.py --data train.json --val val.json --batch-size 4

# Or reduce LoRA rank
python cli/train_teacher.py --data train.json --val val.json --lora-r 8
```

### GroundingDINO or SAM not found

The implementation includes placeholder models for development. To use actual models:

1. Install GroundingDINO: `cd GroundingDINO && pip install -e .`
2. Install SAM: `cd segment_anything && pip install -e .`
3. Download pretrained checkpoints to `data/models/pretrained/`

## Next Steps

After teacher training completes:

1. **Check TensorBoard logs**: `tensorboard --logdir experiments/exp1/logs`
2. **Verify LoRA adapters saved**: `ls experiments/exp1/teachers/`
3. **Proceed to distillation**: `python cli/train_student.py ...`

## Architecture Highlights

### Data-Driven Model Loading

```python
# Inspect ‚Üí Load ‚Üí Train
dataset_info = inspect_dataset(coco_data)

models = {}
if dataset_info['has_boxes']:
    models['grounding_dino'] = load_grounding_dino_with_lora(...)
if dataset_info['has_masks']:
    models['sam'] = load_sam_with_lora(...)

# Train whatever was loaded
for model_name, model in models.items():
    train_model(model_name, model)
```

### LoRA Efficiency

```
Full Fine-tuning:
‚îú‚îÄ Model weights: 11GB
‚îú‚îÄ Gradients: 11GB      ‚Üê All 176M params
‚îú‚îÄ Optimizer: 22GB      ‚Üê Adam: 2√ó (momentum + variance)
‚îî‚îÄ Total: 47GB ‚ùå

LoRA:
‚îú‚îÄ Base weights: 11GB   (frozen ‚ùÑÔ∏è)
‚îú‚îÄ LoRA adapters: 0.019GB (trainable üî•)
‚îú‚îÄ Gradients: 0.019GB   ‚Üê Only 2.5M params!
‚îú‚îÄ Optimizer: 0.038GB   ‚Üê Only for adapters!
‚îî‚îÄ Total: 14.4GB ‚úÖ
```

### Automatic Preprocessing

```python
# Problem: Different models need different preprocessing
# - DINO: 800√ó1333, ImageNet normalization
# - SAM: 1024√ó1024, SAM-specific normalization
# - YOLO: 640√ó640, simple normalization

# Solution: Multi-model preprocessor
preprocessor = MultiModelPreprocessor(
    active_models=['grounding_dino', 'sam']  # From dataset inspection
)

preprocessed = preprocessor.preprocess_batch(image)
# Each model gets correct input automatically!
```

## Performance Expectations

Based on RTX 3090 (24GB):

| Stage | Time | Output |
|-------|------|--------|
| Dataset validation | ~1 min | Validated COCO + splits |
| Grounding DINO LoRA (50 epochs) | 8-12 hours | 19MB adapter |
| SAM LoRA (100 epochs) | 16-24 hours | 1.5MB adapter |
| **Total** | **~1 day** | **20.5MB adapters** |

Compare to full fine-tuning: 3-5 days, 13.4GB checkpoints

## References

- **PEFT Library**: https://github.com/huggingface/peft
- **LoRA Paper**: https://arxiv.org/abs/2106.09685
- **Grounding DINO**: https://arxiv.org/abs/2303.05499
- **SAM**: https://arxiv.org/abs/2304.02643

## Support

For issues:
1. Check `docs/CLI_USAGE.md` for CLI reference
2. See `TECHNICAL_APPROACH.md` for technical details
3. Review `PLATFORM_ARCHITECTURE.md` for architecture
4. Open GitHub issue for bugs


