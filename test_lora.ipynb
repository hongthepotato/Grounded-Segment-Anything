{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6642cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "notebook_path = Path.cwd()\n",
    "sys.path.insert(0, str(notebook_path))\n",
    "\n",
    "from ml_engine.models.teacher.grounding_dino_lora import load_grounding_dino_with_lora\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List\n",
    "import logging\n",
    "import torch\n",
    "from torch import nn\n",
    "from ml_engine.training.peft_utils import (\n",
    "    verify_freezing, save_lora_adapters, apply_lora, load_lora_model\n",
    ")\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util.utils import clean_state_dict\n",
    "\n",
    "from core.constants import DEFAULT_DINO_LORA_CONFIG\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5254f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_checkpoint = \"data/models/pretrained/groundingdino_swint_ogc.pth\"\n",
    "\n",
    "lora_config = {\n",
    "    \"enabled\": True,\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"target_modules\": [\n",
    "        # vision-focused fine-tuning\n",
    "        \"value_proj\", \"output_proj\", \"v_proj\", \"out_v_proj\"\n",
    "        # For text-focused fine-tuning\n",
    "        \"out_proj\", \"l_proj\", \"out_l_proj\"\n",
    "        # Focus cross-model fusion\n",
    "        \"v_proj\", \"l_proj\", \"values_v_proj\", \"values_l_proj\"\n",
    "    ],\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"FEATURE_EXTRACTION\"\n",
    "}\n",
    "\n",
    "bert_model_path = \"data/models/pretrained/bert-base-uncased\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab646954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model(checkpoint_path: str, bert_model_path: str):\n",
    "    \"\"\"\n",
    "    Load pretrained Grounding DINO model.\n",
    "    \n",
    "    This method loads the official GroundingDINO model WITHOUT modification.\n",
    "    Grounding DINO is an open-vocabulary model - it has NO fixed num_classes.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to pretrained checkpoint (.pth file)\n",
    "    \n",
    "    Returns:\n",
    "        Loaded GroundingDINO model\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If checkpoint or config file not found\n",
    "        RuntimeError: If model building or loading fails\n",
    "    \"\"\"\n",
    "    # Add GroundingDINO to path\n",
    "\n",
    "    # Load config\n",
    "    config_file = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "\n",
    "    args = SLConfig.fromfile(str(config_file))\n",
    "\n",
    "    if bert_model_path:\n",
    "        bert_path = Path(bert_model_path)\n",
    "        if not bert_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Local BERT model not found: {bert_path}\\n\"\n",
    "            )\n",
    "        # Set bert_base_uncased_path, NOT text_encoder_type!\n",
    "        args.bert_base_uncased_path = str(bert_path.absolute())\n",
    "    else:\n",
    "        # Ensure bert_base_uncased_path is None for online mode\n",
    "        if not hasattr(args, 'bert_base_uncased_path'):\n",
    "            args.bert_base_uncased_path = None\n",
    "    args.aux_loss = True\n",
    "    \n",
    "    try:\n",
    "        model = build_model(args)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to build Grounding DINO model: {e}\") from e\n",
    "\n",
    "    # Load pretrained checkpoint\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    if not checkpoint_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Checkpoint not found: {checkpoint_path}\\n\"\n",
    "            f\"Download pretrained weights from:\\n\"\n",
    "            f\"  https://github.com/IDEA-Research/GroundingDINO/releases\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load checkpoint: {e}\") from e\n",
    "\n",
    "    # Extract state dict (handle different formats)\n",
    "    if isinstance(checkpoint, dict):\n",
    "        if 'model' in checkpoint:\n",
    "            state_dict = checkpoint['model']\n",
    "            logger.info(\"Checkpoint contains: epoch=%s\", checkpoint.get('epoch', 'N/A'))\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    # Clean and load state dict\n",
    "    try:\n",
    "        state_dict = clean_state_dict(state_dict)\n",
    "        msg = model.load_state_dict(state_dict, strict=False)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load state dict: {e}\") from e\n",
    "\n",
    "    # Report loading results\n",
    "    if msg.missing_keys:\n",
    "        logger.warning(\"Missing keys (%d): %s...\", len(msg.missing_keys), msg.missing_keys[:5])\n",
    "\n",
    "    if msg.unexpected_keys:\n",
    "        logger.warning(\"Unexpected keys (%d): %s...\", len(msg.unexpected_keys), msg.unexpected_keys[:5])\n",
    "\n",
    "    if not msg.missing_keys and not msg.unexpected_keys:\n",
    "        logger.info(\"All keys matched perfectly\")\n",
    "\n",
    "    # Verify model is in correct mode\n",
    "    model.eval()  # Start in eval mode (will be set to train by trainer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c5c3fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use local bert model path: /root/coding/platform/Grounded-Segment-Anything/data/models/pretrained/bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (2): ['label_enc.weight', 'bert.embeddings.position_ids']...\n"
     ]
    }
   ],
   "source": [
    "base_model = load_base_model(base_checkpoint, bert_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f29a6609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use local bert model path: /root/coding/platform/Grounded-Segment-Anything/data/models/pretrained/bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (2): ['label_enc.weight', 'bert.embeddings.position_ids']...\n",
      " Non-LoRA trainable parameters found: ['base_model.model.transformer.decoder.bbox_embed.0.layers.0.weight', 'base_model.model.transformer.decoder.bbox_embed.0.layers.0.bias', 'base_model.model.transformer.decoder.bbox_embed.0.layers.1.weight', 'base_model.model.transformer.decoder.bbox_embed.0.layers.1.bias', 'base_model.model.transformer.decoder.bbox_embed.0.layers.2.weight']...\n"
     ]
    }
   ],
   "source": [
    "final_model = load_grounding_dino_with_lora(\n",
    "    base_checkpoint=base_checkpoint,\n",
    "    lora_config=lora_config,\n",
    "    bert_model_path=bert_model_path\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "256ee2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || \"\n",
    "        f\"all params: {all_param:,} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_param:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f3eedbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 820,740 || all params: 173,527,810 || trainable%: 0.47%\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "181452aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.base_model.model.transformer.encoder.text_layers.0.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.text_layers.0.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.encoder.text_layers.1.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.text_layers.1.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.encoder.text_layers.2.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.text_layers.2.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.encoder.text_layers.3.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.text_layers.3.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.encoder.text_layers.4.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.text_layers.4.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.encoder.text_layers.5.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.text_layers.5.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.0.attn.v_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.0.attn.v_proj.lora_B.default.weight: torch.Size([1024, 16])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.1.attn.v_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.1.attn.v_proj.lora_B.default.weight: torch.Size([1024, 16])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.2.attn.v_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.2.attn.v_proj.lora_B.default.weight: torch.Size([1024, 16])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.3.attn.v_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.3.attn.v_proj.lora_B.default.weight: torch.Size([1024, 16])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.4.attn.v_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.4.attn.v_proj.lora_B.default.weight: torch.Size([1024, 16])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.5.attn.v_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.encoder.fusion_layers.5.attn.v_proj.lora_B.default.weight: torch.Size([1024, 16])\n",
      "model.base_model.model.transformer.decoder.layers.0.ca_text.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.0.ca_text.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.0.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.0.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.1.ca_text.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.1.ca_text.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.1.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.1.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.2.ca_text.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.2.ca_text.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.2.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.2.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.3.ca_text.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.3.ca_text.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.3.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.3.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.4.ca_text.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.4.ca_text.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.4.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.4.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.5.ca_text.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.5.ca_text.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.layers.5.self_attn.out_proj.lora_A.default.weight: torch.Size([16, 256])\n",
      "model.base_model.model.transformer.decoder.layers.5.self_attn.out_proj.lora_B.default.weight: torch.Size([256, 16])\n",
      "model.base_model.model.transformer.decoder.bbox_embed.0.layers.0.weight: torch.Size([256, 256])\n",
      "model.base_model.model.transformer.decoder.bbox_embed.0.layers.0.bias: torch.Size([256])\n",
      "model.base_model.model.transformer.decoder.bbox_embed.0.layers.1.weight: torch.Size([256, 256])\n",
      "model.base_model.model.transformer.decoder.bbox_embed.0.layers.1.bias: torch.Size([256])\n",
      "model.base_model.model.transformer.decoder.bbox_embed.0.layers.2.weight: torch.Size([4, 256])\n",
      "model.base_model.model.transformer.decoder.bbox_embed.0.layers.2.bias: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for name, param in final_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f9daf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All projection layers ===\n",
      "transformer.encoder.layers.0.self_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.0.self_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.1.self_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.1.self_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.2.self_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.2.self_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.3.self_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.3.self_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.4.self_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.4.self_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.5.self_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.layers.5.self_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.text_layers.0.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.text_layers.1.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.text_layers.2.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.text_layers.3.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.text_layers.4.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.text_layers.5.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.0.attn.v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.0.attn.l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.0.attn.values_v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.0.attn.values_l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.0.attn.out_v_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.0.attn.out_l_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.1.attn.v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.1.attn.l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.1.attn.values_v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.1.attn.values_l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.1.attn.out_v_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.1.attn.out_l_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.2.attn.v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.2.attn.l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.2.attn.values_v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.2.attn.values_l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.2.attn.out_v_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.2.attn.out_l_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.3.attn.v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.3.attn.l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.3.attn.values_v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.3.attn.values_l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.3.attn.out_v_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.3.attn.out_l_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.4.attn.v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.4.attn.l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.4.attn.values_v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.4.attn.values_l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.4.attn.out_v_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.4.attn.out_l_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.5.attn.v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.5.attn.l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.5.attn.values_v_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.5.attn.values_l_proj: Linear(in_features=256, out_features=1024, bias=True)\n",
      "transformer.encoder.fusion_layers.5.attn.out_v_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.encoder.fusion_layers.5.attn.out_l_proj: Linear(in_features=1024, out_features=256, bias=True)\n",
      "transformer.decoder.layers.0.cross_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.0.cross_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.0.ca_text.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.0.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.1.cross_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.1.cross_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.1.ca_text.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.1.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.2.cross_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.2.cross_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.2.ca_text.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.2.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.3.cross_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.3.cross_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.3.ca_text.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.3.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.4.cross_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.4.cross_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.4.ca_text.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.4.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.5.cross_attn.value_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.5.cross_attn.output_proj: Linear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.5.ca_text.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "transformer.decoder.layers.5.self_attn.out_proj: NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "input_proj: ModuleList(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  )\n",
      "  (3): Sequential(\n",
      "    (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  )\n",
      ")\n",
      "input_proj.0: Sequential(\n",
      "  (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      ")\n",
      "input_proj.0.0: Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "input_proj.0.1: GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "input_proj.1: Sequential(\n",
      "  (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      ")\n",
      "input_proj.1.0: Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "input_proj.1.1: GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "input_proj.2: Sequential(\n",
      "  (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      ")\n",
      "input_proj.2.0: Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "input_proj.2.1: GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "input_proj.3: Sequential(\n",
      "  (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      ")\n",
      "input_proj.3.0: Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "input_proj.3.1: GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "backbone.0.patch_embed.proj: Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "backbone.0.layers.0.blocks.0.attn.proj: Linear(in_features=96, out_features=96, bias=True)\n",
      "backbone.0.layers.0.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.0.blocks.1.attn.proj: Linear(in_features=96, out_features=96, bias=True)\n",
      "backbone.0.layers.0.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.1.blocks.0.attn.proj: Linear(in_features=192, out_features=192, bias=True)\n",
      "backbone.0.layers.1.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.1.blocks.1.attn.proj: Linear(in_features=192, out_features=192, bias=True)\n",
      "backbone.0.layers.1.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.2.blocks.0.attn.proj: Linear(in_features=384, out_features=384, bias=True)\n",
      "backbone.0.layers.2.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.2.blocks.1.attn.proj: Linear(in_features=384, out_features=384, bias=True)\n",
      "backbone.0.layers.2.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.2.blocks.2.attn.proj: Linear(in_features=384, out_features=384, bias=True)\n",
      "backbone.0.layers.2.blocks.2.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.2.blocks.3.attn.proj: Linear(in_features=384, out_features=384, bias=True)\n",
      "backbone.0.layers.2.blocks.3.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.2.blocks.4.attn.proj: Linear(in_features=384, out_features=384, bias=True)\n",
      "backbone.0.layers.2.blocks.4.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.2.blocks.5.attn.proj: Linear(in_features=384, out_features=384, bias=True)\n",
      "backbone.0.layers.2.blocks.5.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.3.blocks.0.attn.proj: Linear(in_features=768, out_features=768, bias=True)\n",
      "backbone.0.layers.3.blocks.0.attn.proj_drop: Dropout(p=0.0, inplace=False)\n",
      "backbone.0.layers.3.blocks.1.attn.proj: Linear(in_features=768, out_features=768, bias=True)\n",
      "backbone.0.layers.3.blocks.1.attn.proj_drop: Dropout(p=0.0, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "# Find all modules with \"proj\" in their name\n",
    "print(\"=== All projection layers ===\")\n",
    "for name, module in base_model.named_modules():\n",
    "    if \"proj\" in name.lower():\n",
    "        print(f\"{name}: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2aa925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
